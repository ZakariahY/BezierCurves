{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmcolors\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbeziers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers,models\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# DO NOT EDIT. Generated by api_gen.sh\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTypePolicy\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatDTypePolicy\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\api\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\api\\activations\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\activations\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exponential\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gelu\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\activations\\activations.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\__init__.py:9\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# When using the torch backend,\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# torch needs to be imported first, otherwise it will segfault\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# upon import.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m any_symbolic_tensors\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\common\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend_utils\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutocastScope\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasVariable\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\common\\dtypes.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m standardize_dtype\n\u001b[0;32m      7\u001b[0m BOOL_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[0;32m      8\u001b[0m INT_TYPES \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint16\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\common\\variables.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstateless_scope\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_stateless_scope\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstateless_scope\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_stateless_scope\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_name\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mKerasVariable\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_dataset_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_dataset_from_directory\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_file\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\audio_dataset_utils.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_utils\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow_io \u001b[38;5;28;01mas\u001b[39;00m tfio\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\dataset_utils.py:9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPool\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\tree\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_same_structure\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flatten\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_nested\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\tree\\tree_api.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optree\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optree\u001b[38;5;241m.\u001b[39mavailable:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dmtree\u001b[38;5;241m.\u001b[39mavailable:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dmtree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\tree\\optree_impl.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Register backend-specific node classes\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_structures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ListWrapper\n\u001b[0;32m     19\u001b[0m     optree\u001b[38;5;241m.\u001b[39mregister_pytree_node(\n\u001b[0;32m     20\u001b[0m         ListWrapper,\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: (x, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m metadata, children: ListWrapper(\u001b[38;5;28mlist\u001b[39m(children)),\n\u001b[0;32m     23\u001b[0m         namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_nested\u001b[39m(structure):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Create a TF control dependency on the return values of a function.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m  If the function had no return value, a no-op context is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    A context manager.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_spec\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_spec_registry\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops_stack\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_util\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\ops\\array_ops.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flags\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m d_api\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m record\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\dtensor\\python\\api.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Optional, Sequence\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtensor_device\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_dtensor_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layout \u001b[38;5;28;01mas\u001b[39;00m layout_lib\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\dtensor\\python\\dtensor_device.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_util\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m override_binary_operator\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\framework\\override_binary_operator.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor \u001b[38;5;28;01mas\u001b[39;00m tensor_lib\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_math_ops\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m np_dtypes\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op_def_library \u001b[38;5;28;01mas\u001b[39;00m _op_def_library\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated_endpoints\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch \u001b[38;5;28;01mas\u001b[39;00m _dispatch\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeVar, List, Any\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "import matplotlib.colors as mcolors\n",
    "from beziers import *\n",
    "from keras import layers,models\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel,delayed\n",
    "import csv\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pickle\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "import umap\n",
    "from sklearn.metrics import silhouette_score\n",
    "from math import ceil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALISER ET CHARGER LES DONNEES DE DEPART   \n",
    "col_lon = \"LONGITUDE\"\n",
    "col_lat = \"LATITUDE\"\n",
    "col_alti = \"ALTI_STD_FT\"\n",
    "column_names = [col_lon,col_lat]\n",
    "col_groupby = \"SYST_TRAJ_ID\"\n",
    "col_sample = \"SYST_POINT_ID\"\n",
    "\n",
    "df = pd.read_parquet('samples.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ECHANTILLONNE LES DONNEES PAR LONGUEUR DE CORDE\n",
    "\n",
    "def resample_df_by_curve_length(df, col_groupby, col_sample, column_names, number_point=99):\n",
    "    datas = {name: [] for name in [col_groupby, col_sample] + column_names}\n",
    "    groups = df.groupby(col_groupby,sort=True)\n",
    "    \n",
    "    for i, (group, f) in enumerate(groups):\n",
    "        \n",
    "        # Calculer la longueur cumulative le long de la courbe\n",
    "        f = f.sort_values(by=col_sample).reset_index(drop=True)\n",
    "        distances = np.sqrt(sum((f[col].diff() ** 2) for col in column_names))\n",
    "        cumulative_length = distances.cumsum()\n",
    "        # Normaliser la longueur cumulative entre 0 et 100\n",
    "        cumulative_length = 100 * (cumulative_length - cumulative_length.min()) / (cumulative_length.max() - cumulative_length.min())\n",
    "        f['cumulative_length'] = cumulative_length\n",
    "        \n",
    "        # Créer des points rééchantillonnés équidistants le long de la longueur normalisée\n",
    "        r = np.linspace(0, 100, number_point + 1)\n",
    "        \n",
    "        # Ajouter les points de groupe et d'échantillon aux données\n",
    "        datas[col_groupby] += [group] * len(r)\n",
    "        datas[col_sample] += r.tolist()\n",
    "        \n",
    "        # Interpoler chaque colonne pour correspondre aux nouvelles valeurs de longueur\n",
    "        for column_name in column_names:\n",
    "            interp = interp1d(f['cumulative_length'], f[column_name], fill_value=\"extrapolate\")\n",
    "            interpolated_values = list(interp(r).flatten())\n",
    "            datas[column_name] += interpolated_values\n",
    "    \n",
    "    return pd.DataFrame(datas)\n",
    "resampled_df_curve_length = resample_df_by_curve_length(df, col_groupby, col_sample, column_names)\n",
    "resampled_df_curve_length = resampled_df_curve_length.sort_values(by=col_sample).reset_index(drop=True)\n",
    "resampled_df_curve_length[column_names] = resampled_df_curve_length[column_names].interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMET DE VISUALISER LES DIRECTIONS DES TRAJECTOIRES\n",
    "\n",
    "def plot_trajs(df, col_groupby,col_order, col_x, col_y, num_traj_max=10000):\n",
    "    plt.figure(dpi=200)\n",
    "    for i,(traj_id, traj) in enumerate(df.groupby(col_groupby)):\n",
    "        n_colors=len(traj)*2\n",
    "        cmap = mcolors.LinearSegmentedColormap.from_list(\"red_blue\", [\"#FF0000\", \"#0000FF\"], N=n_colors)\n",
    "        traj = traj.sort_values(by=col_order)\n",
    "        colors = [cmap(j / (n_colors-1)) for j in range(0,n_colors,2)]\n",
    "        for j in range(len(traj)-1):\n",
    "            plt.plot(traj[col_x].iloc[j:j+2],traj[col_y].iloc[j:j+2],lw=0.5,color=colors[j] )\n",
    "            if i>=num_traj_max-1:\n",
    "                break\n",
    "    plt.xlabel(col_x)\n",
    "    plt.ylabel(col_y)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "plot_trajs(df,col_groupby,col_order=col_sample,col_x=col_lon,col_y=col_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMET DE VISUALISER INDIVIDUELLEMENT LES TRAJECTOIRES\n",
    "def plot_trajs2(df, col_groupby, col_order, col_x, col_y, selected_traj=None, num_traj_max=10000):\n",
    "    plt.figure(dpi=200)\n",
    "\n",
    "    # Obtenir toutes les trajectoires groupées\n",
    "    grouped_trajs = list(df.groupby(col_groupby))\n",
    "    \n",
    "    # Si une trajectoire spécifique est sélectionnée\n",
    "    if selected_traj is not None:\n",
    "        # Vérifier que l'indice est dans les limites\n",
    "        if selected_traj < len(grouped_trajs):\n",
    "            # Extraire la trajectoire correspondant à l'ordre\n",
    "            traj_id, traj = grouped_trajs[selected_traj]\n",
    "            \n",
    "            # Traiter et tracer cette trajectoire\n",
    "            n_colors = len(traj) * 2\n",
    "            cmap = mcolors.LinearSegmentedColormap.from_list(\"red_blue\", [\"#FF0000\", \"#0000FF\"], N=n_colors)\n",
    "            traj = traj.sort_values(by=col_order)\n",
    "            colors = [cmap(j / (n_colors - 1)) for j in range(0, n_colors, 2)]\n",
    "            for j in range(len(traj) - 1):\n",
    "                plt.plot(traj[col_x].iloc[j:j + 2], traj[col_y].iloc[j:j + 2], lw=0.5, color=colors[j])\n",
    "        else:\n",
    "            print(f\"Indice de trajectoire {selected_traj} hors limites. Nombre total de trajectoires : {len(grouped_trajs)}\")\n",
    "    else:\n",
    "        # Sinon, tracer toutes les trajectoires jusqu'à `num_traj_max`\n",
    "        for i, (traj_id, traj) in enumerate(grouped_trajs):\n",
    "            n_colors = len(traj) * 2\n",
    "            cmap = mcolors.LinearSegmentedColormap.from_list(\"red_blue\", [\"#FF0000\", \"#0000FF\"], N=n_colors)\n",
    "            traj = traj.sort_values(by=col_order)\n",
    "            colors = [cmap(j / (n_colors - 1)) for j in range(0, n_colors, 2)]\n",
    "            for j in range(len(traj) - 1):\n",
    "                plt.plot(traj[col_x].iloc[j:j + 2], traj[col_y].iloc[j:j + 2], lw=0.5, color=colors[j])\n",
    "            if i >= num_traj_max - 1:\n",
    "                break\n",
    "\n",
    "    plt.xlabel(col_x)\n",
    "    plt.ylabel(col_y)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "plot_trajs2(df, col_groupby=col_groupby, col_order=col_sample, col_x=col_lon, col_y=col_lat, selected_traj=432)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMET DE VISUALISER LE RESAMPLING POUR S'ASSURER QU'IL A ETE FAIT CORRECTEMENT\n",
    "\n",
    "def plot_trajs_points(df,df2, col_groupby,col_order, col_x, col_y, num_traj_max=10000):\n",
    "    plt.figure(dpi=200)\n",
    "    for i,(traj_id, traj) in enumerate(df.groupby(col_groupby)):\n",
    "        traj = traj.sort_values(by=col_order)\n",
    "        plt.scatter(traj[col_x], traj[col_y],color=\"b\",s=0.5)\n",
    "        if i>=num_traj_max-1:\n",
    "            break\n",
    "    for i,(traj_id, traj) in enumerate(df2.groupby(col_groupby)):\n",
    "         traj = traj.sort_values(by=col_order)\n",
    "         plt.scatter(traj[col_x], traj[col_y],color=\"r\",s=0.5,alpha=0.5)\n",
    "         if i>=num_traj_max-1:\n",
    "             break\n",
    "    plt.xlabel(col_x)\n",
    "    plt.ylabel(col_y)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "plot_trajs_points(df,resampled_df_curve_length, col_groupby,col_sample, col_lon, col_lat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMET DE VISUALISER LES TRAJECTOIRES APPROXIMEES PAR LES BEZIERS\n",
    "\n",
    "def plot_trajs_beziers(df, col_groupby,col_order, col_x, col_y, num_traj_max=1000):\n",
    "    plt.figure(dpi=200)\n",
    "    groups = df.groupby(col_groupby)\n",
    "    for i,(traj_id, traj) in enumerate(groups):\n",
    "        traj = traj.sort_values(by=col_order)\n",
    "        points = traj[['LONGITUDE', 'LATITUDE']].to_numpy()\n",
    "        beziers = fit_curve(points)\n",
    "        control_points=get_control_points(beziers)\n",
    "        print(len(control_points))\n",
    "        for curve in beziers:\n",
    "            curve = np.array(curve)  \n",
    "            t_values = np.linspace(0, 1, 100)\n",
    "            bezier_points = np.array([bezier_q(curve, t) for t in t_values]) \n",
    "            print(curve)\n",
    "            plt.plot(bezier_points[:, 0], bezier_points[:, 1], color=\"r\")\n",
    "        for cp in control_points:\n",
    "            plt.scatter(cp[0],cp[1],color='green',s=4)\n",
    "        plt.plot(traj[col_x], traj[col_y],color=\"b\",alpha=0.5)\n",
    "        if i>=num_traj_max-1:\n",
    "            break\n",
    "    plt.xlabel(col_x)\n",
    "    plt.ylabel(col_y)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "plot_trajs_beziers(resampled_df_curve_length,col_groupby=col_groupby,col_order=col_sample,col_x=col_lon,col_y=col_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMET DE CALCULER LES POINTS DE CONTROLES DE BEZIERS POUR CHAQUE TRAJECTOIRE ET DES LES SAUVEGARDER\n",
    "def get_beziers_traj(df, col_groupby, number_traj=1000):\n",
    "    groups = df.groupby(col_groupby,sort=True)\n",
    "    selected_groups = groups\n",
    "    def bezier_from_group(group):\n",
    "          \n",
    "        points = group[['LONGITUDE', 'LATITUDE']].to_numpy()\n",
    "        beziers = fit_curve(points)\n",
    "        control_points = get_control_points(beziers)\n",
    "        print(control_points)\n",
    "        return np.array(control_points).flatten()\n",
    "    control_points_list=Parallel(n_jobs=-1)(delayed(bezier_from_group)(group) for traj_id, group in selected_groups )\n",
    "    return control_points_list\n",
    "cp=get_beziers_traj(resampled_df_curve_length,col_groupby,number_traj=1000)\n",
    "with open('data10.csv', 'w', newline='') as f:\n",
    "      writer = csv.writer(f)\n",
    "      writer.writerows(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMET DE RECHARGER LES POINTS DE CONTROLES\n",
    "cp = []\n",
    "with open('data10.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "         control_points = np.array(row, dtype=float)\n",
    "         cp.append(control_points)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREER LE MODELE D'AUTOENCODER, ENCODER ET DECODER\n",
    "def create_autoencoder(input_dim, pregu=0.0001, perte='mean_squared_error', n=1, learning_rate=0.0001):\n",
    " \n",
    "    # Encodeur\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    encoded = layers.Dense(n*128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(pregu))(input_layer)\n",
    "    encoded = layers.Dense(n*64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(pregu))(encoded)\n",
    "    encoded = layers.Dense(n*32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(pregu))(encoded)\n",
    "\n",
    "\n",
    "    latent_space = layers.Dense(n*16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(pregu))(encoded)\n",
    "\n",
    "    # Décodeur\n",
    "    latent_input = layers.Input(shape=(n*16,))\n",
    "    decoded = layers.Dense(n*32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(pregu))(latent_input)\n",
    "    decoded = layers.Dense(n*64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(pregu))(decoded)\n",
    "    decoded = layers.Dense(n*128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(pregu))(decoded)\n",
    "    output_layer = layers.Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "    # Modèles\n",
    "    encoder = models.Model(input_layer, latent_space, name=\"encoder\")\n",
    "    decoder = models.Model(latent_input, output_layer, name=\"decoder\")\n",
    "    autoencoder = models.Model(input_layer, decoder(encoder(input_layer)), name=\"autoencoder\")\n",
    "\n",
    "    # Compilation\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    autoencoder.compile(optimizer=optimizer, loss=perte)\n",
    "\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERTE PERSONNALISE POUR L'AUTOENCODER\n",
    "def custom_loss(weight_points=5.0):\n",
    "    def loss_function(y_true, y_pred):\n",
    "        \n",
    "          \n",
    "\n",
    "            # Calcul des différentes pertes\n",
    "        points_loss = tf.reduce_mean(\n",
    "                tf.square(y_true-y_pred\n",
    "            ))\n",
    "\n",
    "           \n",
    "            \n",
    "            # Ajout au total\n",
    "        total_loss = points_loss*weight_points\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    return loss_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTRAINE L'AUTOENCODER ET GARDE LE MEILLEUR MODELE ET AFFICHE LES COURBES DE LOSS\n",
    "\n",
    "def train_autoencoder(cp, epochs,n,pregu,learning_rate,perte='mean_squared_loss'):\n",
    "    cp=np.array(cp)\n",
    "    # Normalisation des données\n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(cp)\n",
    "    input_dim = X_normalized.shape[1]\n",
    "    print(input_dim)\n",
    "    # Création de l'autoencodeur\n",
    "    autoencoder, encoder, decoder = create_autoencoder(input_dim,perte=perte,n=n,pregu=pregu,learning_rate=learning_rate)\n",
    "    \n",
    "    # Division des données en ensemble d'entraînement et de validation\n",
    "    X_train, X_val = train_test_split(X_normalized, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Callback pour sauvegarder le meilleur modèle\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'best_autoencoder.keras', \n",
    "        monitor='val_loss', \n",
    "        save_best_only=True, \n",
    "        mode='min', \n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Entraînement\n",
    "    history = autoencoder.fit(\n",
    "        X_train,\n",
    "        X_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, X_val),\n",
    "        callbacks=[checkpoint],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Charger le meilleur modèle\n",
    "    autoencoder.load_weights('best_autoencoder.keras')\n",
    "    \n",
    "    # Tracer les courbes de perte\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Loss (Training)')\n",
    "    plt.plot(history.history['val_loss'], label='Loss (Validation)')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    return autoencoder, encoder, decoder, X_val, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AFFICHE LES TRAJECTOIRES RECONSTRUITES DU SET DE VALIDATION ET CALCULE L'ERREUR ASSOCIEE\n",
    "def reconstruct_bez(autoencoder, X_val, scaler):\n",
    "    X_reconstructed = autoencoder.predict(X_val)\n",
    "    X_normalized = scaler.inverse_transform(X_reconstructed)\n",
    "    X_original = scaler.inverse_transform(X_val)\n",
    "\n",
    "    original1=[]\n",
    "    reconstructed1=[]\n",
    "    # Trajectoires originales et reconstruites\n",
    "    all_trajectoire_original = []\n",
    "    all_trajectoire_reconstructed = []\n",
    "\n",
    "    for original, reconstructed in zip(X_original, X_normalized):\n",
    "        traj_original, traj_reconstructed = [], []\n",
    "        print(len(original))\n",
    "        # Gestion selon la taille de la liste\n",
    "        indices = list(range(8))\n",
    "        step=8\n",
    "        # Extraction et reconstruction\n",
    "        for k in range(0, len(original), step):\n",
    "            orig_packet = [original[k + i] for i in indices if k + i < len(original)]\n",
    "            rec_packet = [reconstructed[k + i] for i in indices if k + i < len(reconstructed)]\n",
    "            if k > 0 and len(orig_packet) == len(indices):\n",
    "                orig_packet[:2] = traj_original[-1][-1]  # Dernière courbe de la trajectoire précédente\n",
    "                rec_packet[:2] = traj_reconstructed[-1][-1]  # Idem pour la trajectoire reconstruite\n",
    "            if len(orig_packet) == len(indices):\n",
    "                # Crée les points de Bézier\n",
    "                bez_orig = [orig_packet[j:j + 2] for j in range(0, len(orig_packet), 2)]\n",
    "                bez_rec = [rec_packet[j:j + 2] for j in range(0, len(rec_packet), 2)]\n",
    "                \n",
    "                traj_original.append(bez_orig)\n",
    "                traj_reconstructed.append(bez_rec)\n",
    "\n",
    "        all_trajectoire_original.append(traj_original)\n",
    "        all_trajectoire_reconstructed.append(traj_reconstructed)\n",
    "\n",
    "    # Calcul de l'erreur L2\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for traj_original, traj_reconstructed in zip(all_trajectoire_original, all_trajectoire_reconstructed):\n",
    "        original_traj = []\n",
    "        reconstructed_traj = []\n",
    "        for curve_original, curve_reconstructed in zip(traj_original, traj_reconstructed):\n",
    "            t_values = np.linspace(0, 1,1000)\n",
    "\n",
    "            # Calcul des points sur les courbes originales et reconstruites\n",
    "            bezier_points_original = np.array([bezier_q(curve_original, t) for t in t_values])\n",
    "            bezier_points_reconstructed = np.array([bezier_q(curve_reconstructed, t) for t in t_values])\n",
    "\n",
    "            # Accumulation des points\n",
    "            original_traj.append(bezier_points_original)\n",
    "            reconstructed_traj.append(bezier_points_reconstructed)\n",
    "        \n",
    "    # Calcul de l'erreur L2 globale\n",
    "        original1.append(original_traj)\n",
    "        reconstructed1.append(reconstructed_traj)\n",
    "    \n",
    "    l2_error=np.linalg.norm(np.array(original1)-np.array(reconstructed1),axis=1).mean()\n",
    "    i=0\n",
    "    for traj_original, traj_reconstructed in zip(all_trajectoire_original, all_trajectoire_reconstructed):\n",
    "        for curve_original, curve_reconstructed in zip(traj_original, traj_reconstructed):\n",
    "            curve_original = np.array(curve_original)\n",
    "            curve_reconstructed = np.array(curve_reconstructed)\n",
    "\n",
    "            t_values = np.linspace(0, 1, 1000)\n",
    "            bezier_points_original = np.array([bezier_q(curve_original, t) for t in t_values])\n",
    "            bezier_points_reconstructed = np.array([bezier_q(curve_reconstructed, t) for t in t_values])\n",
    "\n",
    "            plt.plot(bezier_points_original[:, 0], bezier_points_original[:, 1], color='b', label='Original', alpha=0.7)\n",
    "            plt.plot(bezier_points_reconstructed[:, 0], bezier_points_reconstructed[:, 1], color='r', label='Reconstruit', alpha=0.7)\n",
    "        i+=1\n",
    "        if i  > 4:\n",
    "            break\n",
    "    plt.xlabel('LONGITUDE')\n",
    "    plt.ylabel('LATITUDE')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "    print(f\"Erreur moyenne L2 : {l2_error}\")\n",
    "    return l2_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAUVAGERDE LES MODELES D'AUTOENCODER,ENCODER ET DECODER\n",
    "\n",
    "@register_keras_serializable()\n",
    "def loss_function(weight_points=10.0):\n",
    "    def loss_function(y_true, y_pred):\n",
    "        \n",
    "          \n",
    "\n",
    "            # Calcul des différentes pertes\n",
    "        points_loss = tf.reduce_mean(\n",
    "                tf.square(y_true-y_pred\n",
    "            ))\n",
    "\n",
    "           \n",
    "            \n",
    "            # Ajout au total\n",
    "        total_loss = points_loss*weight_points\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "save_dir = \"models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Sauvegarder l'autoencodeur complet\n",
    "autoencoder.save(os.path.join(save_dir, \"autoencoder4.keras\"))\n",
    "\n",
    "# Sauvegarder les parties encoder et decoder individuellement\n",
    "encoder.save(os.path.join(save_dir, \"encoder4.keras\"))\n",
    "decoder.save(os.path.join(save_dir, \"decoder4.keras\"))\n",
    "\n",
    "# Sauvegarder les données transformées X_valt et scaler\n",
    "with open(os.path.join(save_dir, \"X_val4.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(X_val, f)\n",
    "\n",
    "with open(os.path.join(save_dir, \"scaler4.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHARGE LES MODELES\n",
    "autoencoder = load_model(os.path.join(save_dir, \"autoencoder4.keras\"))\n",
    "encoder = load_model(os.path.join(save_dir, \"encoder4.keras\"))\n",
    "decoder = load_model(os.path.join(save_dir, \"decoder4.keras\"))\n",
    "\n",
    "with open(os.path.join(save_dir, \"X_val4.pkl\"), \"rb\") as f:\n",
    "    X_val = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(save_dir, \"scaler4.pkl\"), \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLUSTERISE LES TRAJECTOIRES AVEC BICRCH\n",
    "def cluster_birch(df, premiers_points, points_barycentres, barycentres, n_clusters):\n",
    "    \n",
    "    features = np.hstack([premiers_points, points_barycentres])\n",
    "    \n",
    "    \n",
    "    birch = Birch(threshold=0.5,n_clusters=n_clusters) \n",
    "    birch.fit(features)\n",
    "    cluster_labels = birch.labels_  \n",
    "    \n",
    "    \n",
    "    barycentres['Cluster'] = cluster_labels\n",
    "\n",
    "    df_sorted = df.sort_values(by=['SYST_TRAJ_ID', 'SYST_POINT_ID'])\n",
    "    df_clusters = df_sorted.merge(barycentres[['SYST_TRAJ_ID', 'Cluster']], on='SYST_TRAJ_ID', how='left')\n",
    "\n",
    " \n",
    "    cmap = plt.get_cmap('tab10', n_clusters)  \n",
    "    cluster_colors = {cluster: cmap(cluster) for cluster in range(n_clusters)}\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_data = df_clusters[df_clusters['Cluster'] == cluster]\n",
    "        for traj_id, traj_points in cluster_data.groupby('SYST_TRAJ_ID'):\n",
    "            plt.plot(\n",
    "                traj_points['LONGITUDE'], \n",
    "                traj_points['LATITUDE'], \n",
    "                color=cluster_colors[cluster],  \n",
    "                label=f\"Cluster {cluster}\" if traj_id == cluster_data['SYST_TRAJ_ID'].iloc[0] else \"\", \n",
    "                alpha=0.6\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title('Trajectoires complètes avec Clustering (Birch)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(n_clusters)\n",
    "    return df_clusters,cluster_colors\n",
    "\n",
    "premiers_points = resampled_df_curve_length.loc[resampled_df_curve_length.groupby('SYST_TRAJ_ID')['SYST_POINT_ID'].idxmin()].sort_values(by='SYST_TRAJ_ID')[['LONGITUDE', 'LATITUDE']].to_numpy()\n",
    "derniers_points = resampled_df_curve_length.sort_values(by=['SYST_TRAJ_ID', 'SYST_POINT_ID']).groupby('SYST_TRAJ_ID').tail(15)\n",
    "barycentres = derniers_points.groupby('SYST_TRAJ_ID')[['LATITUDE', 'LONGITUDE']].mean().reset_index()\n",
    "points_barycentres = barycentres[['LONGITUDE', 'LATITUDE']].to_numpy()\n",
    "syst_traj_ids = barycentres['SYST_TRAJ_ID'].to_numpy()\n",
    "\n",
    "df_clusters,cluster_colors=cluster_birch(\n",
    "    df=resampled_df_curve_length, \n",
    "    premiers_points=premiers_points, \n",
    "    points_barycentres=points_barycentres, \n",
    "    barycentres=barycentres, \n",
    "    n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ASSSOCIE A CHAQUE ID DE TRAJ UN CLUSTER\n",
    "id_cluster_mapping = (\n",
    "    df_clusters.groupby('SYST_TRAJ_ID', sort=True)['Cluster']\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROJETTE L'ESPACE LATENT EN 2D\n",
    "\n",
    "cp = np.array(cp)\n",
    "X_normalized = scaler.fit_transform(cp)\n",
    "X_latent = encoder.predict(X_normalized)\n",
    "\n",
    "\n",
    "reducer = umap.UMAP(n_components=2, n_neighbors=50, min_dist=0.01, metric='euclidean', random_state=42)\n",
    "X_umap = reducer.fit_transform(X_latent)\n",
    "\n",
    "df_umap = pd.DataFrame(X_umap, columns=['Dim1', 'Dim2'])\n",
    "df_umap['Cluster'] = id_cluster_mapping['Cluster'].values\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(\n",
    "    df_umap['Dim1'],\n",
    "    df_umap['Dim2'],\n",
    "    c=df_umap['Cluster'].map(lambda cluster: cluster_colors[cluster]),\n",
    "    s=50,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Ajout des légendes\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Projection UMAP colorée par Cluster (paramètres optimaux)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=50, learning_rate=10, n_iter=1000, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_latent)\n",
    "\n",
    "df_tsne = pd.DataFrame(X_tsne, columns=['Dim1', 'Dim2'])\n",
    "df_tsne['Cluster'] = id_cluster_mapping['Cluster'].values\n",
    "\n",
    "# Visualisation des résultats t-SNE\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(\n",
    "    df_tsne['Dim1'],\n",
    "    df_tsne['Dim2'],\n",
    "    c=df_tsne['Cluster'].map(lambda cluster: cluster_colors[cluster]),\n",
    "    s=50,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('Projection t-SNE (paramètres optimisés)')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour optimiser les paramètres de DBSCAN\n",
    "def optimize_dbscan(X, eps_values, min_samples_values):\n",
    "    best_params = None\n",
    "    best_score = -1\n",
    "    results = []\n",
    "\n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(X)\n",
    "\n",
    "            # Filtrer les outliers (label = -1)\n",
    "            if len(set(labels)) > 1 and -1 not in set(labels):\n",
    "                score = silhouette_score(X, labels)\n",
    "                results.append((eps, min_samples, score))\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = (eps, min_samples)\n",
    "\n",
    "    return best_params, best_score, results\n",
    "\n",
    "# Paramètres à tester pour DBSCAN\n",
    "eps_values = [5.0]\n",
    "min_samples_values = [20,30,40,50]\n",
    "\n",
    "# Optimisation de DBSCAN pour UMAP\n",
    "best_params_umap, best_score_umap, dbscan_results_umap = optimize_dbscan(X_umap, eps_values, min_samples_values)\n",
    "print(f\"Meilleurs paramètres DBSCAN (UMAP): eps={best_params_umap[0]}, min_samples={best_params_umap[1]} avec un score silhouette de {best_score_umap:.4f}\")\n",
    "\n",
    "# Clustering DBSCAN avec les meilleurs paramètres pour UMAP\n",
    "dbscan_umap = DBSCAN(eps=best_params_umap[0], min_samples=best_params_umap[1])\n",
    "labels_umap = dbscan_umap.fit_predict(X_umap)\n",
    "\n",
    "# Optimisation de DBSCAN pour t-SNE\n",
    "best_params_tsne, best_score_tsne, dbscan_results_tsne = optimize_dbscan(X_tsne, eps_values, min_samples_values)\n",
    "print(f\"Meilleurs paramètres DBSCAN (t-SNE): eps={best_params_tsne[0]}, min_samples={best_params_tsne[1]} avec un score silhouette de {best_score_tsne:.4f}\")\n",
    "\n",
    "# Clustering DBSCAN avec les meilleurs paramètres pour t-SNE\n",
    "dbscan_tsne = DBSCAN(eps=best_params_tsne[0], min_samples=best_params_tsne[1])\n",
    "labels_tsne = dbscan_tsne.fit_predict(X_tsne)\n",
    "\n",
    "# Génération du dictionnaire des couleurs\n",
    "def generate_cluster_colors(labels, cmap_name='tab20'):\n",
    "    unique_labels = set(labels)\n",
    "    cmap = plt.get_cmap(cmap_name, len(unique_labels))\n",
    "    cluster_colors = {label: cmap(i) for i, label in enumerate(sorted(unique_labels))}\n",
    "    return cluster_colors\n",
    "\n",
    "# Génération des couleurs pour UMAP et t-SNE\n",
    "umap_colors = generate_cluster_colors(labels_umap)\n",
    "tsne_colors = generate_cluster_colors(labels_tsne)\n",
    "\n",
    "# Visualisation des clusters UMAP avec couleurs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(\n",
    "    df_umap['Dim1'],\n",
    "    df_umap['Dim2'],\n",
    "    c=[umap_colors[label] for label in labels_umap],\n",
    "    s=50,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('DBSCAN Clustering (UMAP)')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "\n",
    "# Visualisation des clusters t-SNE avec couleurs\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(\n",
    "    df_tsne['Dim1'],\n",
    "    df_tsne['Dim2'],\n",
    "    c=[tsne_colors[label] for label in labels_tsne],\n",
    "    s=50,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('DBSCAN Clustering (t-SNE)')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Création des DataFrames finaux contenant les clusters et indices des trajectoires\n",
    "df_umap_clusters = pd.DataFrame({\n",
    "    'Latent_Index': range(len(X_umap)),\n",
    "    'UMAP_Cluster': labels_umap\n",
    "})\n",
    "\n",
    "df_tsne_clusters = pd.DataFrame({\n",
    "    'Latent_Index': range(len(X_tsne)),\n",
    "    'tSNE_Cluster': labels_tsne\n",
    "})\n",
    "\n",
    "# Affichage des DataFrames finaux contenant les clusters\n",
    "print(\"Clusters DBSCAN pour UMAP :\")\n",
    "print(df_umap_clusters.head())\n",
    "\n",
    "print(\"\\nClusters DBSCAN pour t-SNE :\")\n",
    "print(df_tsne_clusters.head())\n",
    "\n",
    "# Enregistrement des DataFrames dans des fichiers CSV pour les consulter ultérieurement\n",
    "df_umap_clusters.to_csv(\"umap_clusters.csv\", index=False)\n",
    "df_tsne_clusters.to_csv(\"tsne_clusters.csv\", index=False)\n",
    "\n",
    "print(\"\\nLes résultats ont été enregistrés dans 'umap_clusters.csv' et 'tsne_clusters.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories_umap_tsne(df, traj_labels, col_traj_id, col_x, col_y, cluster_colors, title=\"Trajectoires colorées par cluster\"):\n",
    "\n",
    "    # Trier le DataFrame par l'ID des trajectoires\n",
    "    df_sorted = df.sort_values(by=['SYST_TRAJ_ID', 'SYST_POINT_ID'])\n",
    "\n",
    "    # Extraire les IDs de trajectoire uniques\n",
    "    unique_traj_ids = df_sorted[col_traj_id].unique()\n",
    "\n",
    "    # Vérifier que la taille des labels correspond aux IDs uniques\n",
    "    if len(traj_labels) != len(unique_traj_ids):\n",
    "        raise ValueError(f\"Le nombre de labels ({len(traj_labels)}) ne correspond pas au nombre d'IDs uniques ({len(unique_traj_ids)}).\")\n",
    "\n",
    "    # Créer un DataFrame des clusters pour lier les labels\n",
    "    cluster_mapping = pd.DataFrame({\n",
    "        col_traj_id: unique_traj_ids,\n",
    "        'Cluster': traj_labels\n",
    "    })\n",
    "\n",
    "    # Associer les clusters au DataFrame principal\n",
    "    df_with_clusters = df_sorted.merge(cluster_mapping, on=col_traj_id, how='left')\n",
    "\n",
    "    # Affichage des trajectoires\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for cluster in sorted(df_with_clusters['Cluster'].unique()):\n",
    "        cluster_data = df_with_clusters[df_with_clusters['Cluster'] == cluster]\n",
    "        color = cluster_colors.get(cluster, 'gray')  # Couleur définie pour chaque cluster ou gris pour les outliers\n",
    "        for traj_id, traj_points in cluster_data.groupby(col_traj_id):\n",
    "            plt.plot(\n",
    "                traj_points[col_x],\n",
    "                traj_points[col_y],\n",
    "                color=color,\n",
    "                label=f\"Cluster {cluster}\" if traj_id == cluster_data[col_traj_id].iloc[0] else \"\",\n",
    "                alpha=0.6,\n",
    "                lw=1.5\n",
    "            )\n",
    "\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return df_with_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_clusters = plot_trajectories_umap_tsne(\n",
    "    df=resampled_df_curve_length,\n",
    "    traj_labels=labels_umap,\n",
    "    col_traj_id='SYST_TRAJ_ID',\n",
    "    col_x='LONGITUDE',\n",
    "    col_y='LATITUDE',\n",
    "    cluster_colors=umap_colors,\n",
    "    title=\"Trajectoires colorées par clusters (UMAP + DBSCAN)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories_by_cluster_in_subplots(df, traj_labels, col_traj_id, col_x, col_y, cluster_colors, output_path=None):\n",
    "\n",
    "    # Sort DataFrame for smoother trajectory plotting\n",
    "    df_sorted = df.sort_values(by=['SYST_TRAJ_ID', 'SYST_POINT_ID'])\n",
    "\n",
    "    # Get unique trajectory IDs\n",
    "    unique_traj_ids = df_sorted[col_traj_id].unique()\n",
    "\n",
    "    # Check consistency between labels and unique trajectory IDs\n",
    "    if len(traj_labels) != len(unique_traj_ids):\n",
    "        raise ValueError(f\"The number of labels ({len(traj_labels)}) does not match the number of unique trajectory IDs ({len(unique_traj_ids)}).\")\n",
    "\n",
    "    # Map cluster labels to trajectory IDs\n",
    "    cluster_mapping = pd.DataFrame({\n",
    "        col_traj_id: unique_traj_ids,\n",
    "        'Cluster': traj_labels\n",
    "    })\n",
    "\n",
    "    # Merge cluster information with the main DataFrame\n",
    "    df_with_clusters = df_sorted.merge(cluster_mapping, on=col_traj_id, how='left')\n",
    "\n",
    "    # Prepare clusters and subplots\n",
    "    clusters = sorted(df_with_clusters['Cluster'].unique())\n",
    "    n_clusters = len(clusters)\n",
    "    n_cols = 3  # Number of columns in subplots\n",
    "    n_rows = ceil(n_clusters / n_cols)  # Calculate rows based on number of clusters\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), constrained_layout=True)\n",
    "    axes = axes.flatten()  # Flatten axes for easier iteration\n",
    "\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        ax = axes[i]\n",
    "        cluster_data = df_with_clusters[df_with_clusters['Cluster'] == cluster]\n",
    "        color = cluster_colors.get(cluster, 'gray')\n",
    "\n",
    "        for traj_id, traj_points in cluster_data.groupby(col_traj_id):\n",
    "            ax.plot(\n",
    "                traj_points[col_x],\n",
    "                traj_points[col_y],\n",
    "                color=color,\n",
    "                alpha=0.5,\n",
    "                lw=0.8\n",
    "            )\n",
    "\n",
    "        # Subplot settings\n",
    "        ax.set_title(f'Cluster {cluster}')\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Hide empty subplots (if any)\n",
    "    for j in range(len(clusters), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Add a global legend\n",
    "    handles = [plt.Line2D([], [], color=cluster_colors[cluster], label=f'Cluster {cluster}')\n",
    "               for cluster in clusters]\n",
    "    fig.legend(handles=handles, title=\"Clusters\", loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=n_cols)\n",
    "\n",
    "    # Save or display the figure\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectories_by_cluster_in_subplots(\n",
    "    df=resampled_df_curve_length,\n",
    "    traj_labels=labels_umap,\n",
    "    col_traj_id='SYST_TRAJ_ID',\n",
    "    col_x='LONGITUDE',\n",
    "    col_y='LATITUDE',\n",
    "    cluster_colors=umap_colors,\n",
    "    output_path='trajectories_by_cluster_subplots.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Réduction de dimension avec PCA ###\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_latent)\n",
    "\n",
    "### Réduction de dimension avec t-SNE ###\n",
    "\n",
    "\n",
    "# Création des DataFrames pour chaque méthode\n",
    "df_pca = pd.DataFrame(X_pca, columns=['Dim1', 'Dim2'])\n",
    "df_pca['Cluster'] = id_cluster_mapping['Cluster'].values\n",
    "\n",
    "\n",
    "\n",
    "### Visualisation PCA ###\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter_pca = plt.scatter(\n",
    "    df_pca['Dim1'], \n",
    "    df_pca['Dim2'], \n",
    "    c=df_pca['Cluster'].map(lambda cluster: cluster_colors[cluster]),  # Couleur selon le cluster,         # Colormap discrète pour les clusters\n",
    "    s=50,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "\n",
    "# Configuration des axes et affichage\n",
    "plt.title(\"Projection PCA\")\n",
    "plt.legend()\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Projection PCA colorée par Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALISER LES TRAJECTOIRES RECONSTRUITES PAR ID POUR INVESTIGUER LES ERREUR DE RECONSTRUCTIONS\n",
    "def reconstruct_bez_id(cp, X_val, scaler, selected_ids):\n",
    "    print(len(X_val))\n",
    "    # Sélectionner uniquement les trajectoires correspondant aux indices donnés\n",
    "    X_val_selected = X_val[selected_ids]\n",
    "    \n",
    "    # Reconstruction avec l'autoencodeur\n",
    "    X_normalized = scaler.inverse_transform(X_val_selected)\n",
    "    X_original = cp[selected_ids]\n",
    "\n",
    "    original1 = []\n",
    "    reconstructed1 = []\n",
    "    all_trajectoire_original = []\n",
    "    all_trajectoire_reconstructed = []\n",
    "\n",
    "    for original, reconstructed in zip(X_original, X_normalized):\n",
    "        traj_original, traj_reconstructed = [], []\n",
    "        indices = list(range(8))\n",
    "        step = 8\n",
    "\n",
    "        for k in range(0, len(original), step):\n",
    "            orig_packet = [original[k + i] for i in indices if k + i < len(original)]\n",
    "            rec_packet = [reconstructed[k + i] for i in indices if k + i < len(reconstructed)]\n",
    "            \n",
    "            # Remplacer les deux premiers éléments du paquet par les deux précédents, si possible\n",
    "            if k > 0 and len(orig_packet) == len(indices):\n",
    "                orig_packet[:2] = traj_original[-1][-1]  # Dernière courbe de la trajectoire précédente\n",
    "                rec_packet[:2] = traj_reconstructed[-1][-1]  # Idem pour la trajectoire reconstruite\n",
    "\n",
    "            if len(orig_packet) == len(indices):\n",
    "                bez_orig = [orig_packet[j:j + 2] for j in range(0, len(orig_packet), 2)]\n",
    "                bez_rec = [rec_packet[j:j + 2] for j in range(0, len(rec_packet), 2)]\n",
    "                \n",
    "                traj_original.append(bez_orig)\n",
    "                traj_reconstructed.append(bez_rec)\n",
    "\n",
    "        all_trajectoire_original.append(traj_original)\n",
    "        all_trajectoire_reconstructed.append(traj_reconstructed)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for traj_original, traj_reconstructed in zip(all_trajectoire_original, all_trajectoire_reconstructed):\n",
    "        original_traj = []\n",
    "        reconstructed_traj = []\n",
    "        for curve_original, curve_reconstructed in zip(traj_original, traj_reconstructed):\n",
    "            t_values = np.linspace(0, 1, 1000)\n",
    "\n",
    "            bezier_points_original = np.array([bezier_q(curve_original, t) for t in t_values])\n",
    "            bezier_points_reconstructed = np.array([bezier_q(curve_reconstructed, t) for t in t_values])\n",
    "\n",
    "            original_traj.append(bezier_points_original)\n",
    "            reconstructed_traj.append(bezier_points_reconstructed)\n",
    "\n",
    "        original1.append(original_traj)\n",
    "        reconstructed1.append(reconstructed_traj)\n",
    "        \n",
    "    l2_error = np.linalg.norm(np.array(original1) - np.array(reconstructed1), axis=1).mean()\n",
    "    \n",
    "    # Tracé des trajectoires sélectionnées\n",
    "    for traj_original, traj_reconstructed in zip(all_trajectoire_original, all_trajectoire_reconstructed):\n",
    "        for curve_original, curve_reconstructed in zip(traj_original, traj_reconstructed):\n",
    "            curve_original = np.array(curve_original)\n",
    "            curve_reconstructed = np.array(curve_reconstructed)\n",
    "\n",
    "            t_values = np.linspace(0, 1, 1000)\n",
    "            bezier_points_original = np.array([bezier_q(curve_original, t) for t in t_values])\n",
    "            bezier_points_reconstructed = np.array([bezier_q(curve_reconstructed, t) for t in t_values])\n",
    "\n",
    "            plt.plot(bezier_points_original[:, 0], bezier_points_original[:, 1], color='b', label='Original', alpha=0.7)\n",
    "            plt.plot(bezier_points_reconstructed[:, 0], bezier_points_reconstructed[:, 1], color='r', label='Reconstruit', alpha=0.7)\n",
    "\n",
    "    plt.xlabel('LONGITUDE')\n",
    "    plt.ylabel('LATITUDE')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.title(\"Trajectoires sélectionnées : Originales vs Reconstruites\")\n",
    "    plt.legend(['Original', 'Reconstruit'])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Erreur moyenne L2 pour les trajectoires sélectionnées : {l2_error}\")\n",
    "    return l2_error\n",
    "X_decoded=decoder.predict(X_latent)\n",
    "reconstruct_bez_id(cp,autoencoder, X_decoded,scaler,selected_ids=[695])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULE LES ERREURS POUR CHAQUE TRAJECTOIRE\n",
    "\n",
    "def errors_df(cp, X_decoded, scaler):\n",
    "    X_original = cp\n",
    "    X_decoded = scaler.inverse_transform(X_decoded)\n",
    "    \n",
    "\n",
    "    errors = []\n",
    "    indices = []\n",
    "\n",
    "    for idx, (original, reconstructed) in enumerate(zip(X_original, X_decoded)):\n",
    "        traj_original, traj_reconstructed = [], []\n",
    "        step = 8  # Taille de chaque courbe Bézier (4 points de contrôle)\n",
    "\n",
    "        for k in range(0, len(original), step):\n",
    "            orig_packet = original[k:k + step]\n",
    "            rec_packet = reconstructed[k:k + step]\n",
    "            if k > 0 and len(orig_packet) == len(indices):\n",
    "                orig_packet[:2] = traj_original[-1][-1]  # Dernière courbe de la trajectoire précédente\n",
    "                rec_packet[:2] = traj_reconstructed[-1][-1]  # Idem pour la trajectoire reconstruite\n",
    "\n",
    "            if len(orig_packet) == step:\n",
    "                bez_orig = [orig_packet[j:j + 2] for j in range(0, len(orig_packet), 2)]\n",
    "                bez_rec = [rec_packet[j:j + 2] for j in range(0, len(rec_packet), 2)]\n",
    "\n",
    "                traj_original.append(bez_orig)\n",
    "                traj_reconstructed.append(bez_rec)\n",
    "\n",
    "        # Calcul des points sur les courbes et de l'erreur pour cette trajectoire\n",
    "        t_values = np.linspace(0, 1, 1000)\n",
    "        original_points = np.concatenate(\n",
    "            [np.array([bezier_q(curve, t) for t in t_values]) for curve in traj_original], axis=0\n",
    "        )\n",
    "        reconstructed_points = np.concatenate(\n",
    "            [np.array([bezier_q(curve, t) for t in t_values]) for curve in traj_reconstructed], axis=0\n",
    "        )\n",
    "        \n",
    "        # Calcul de l'erreur moyenne L2 pour la trajectoire\n",
    "        l2_error = np.linalg.norm(np.array(original_points)- np.array(reconstructed_points), axis=1).mean()\n",
    "\n",
    "        errors.append(l2_error)\n",
    "        indices.append(idx)\n",
    "\n",
    "    # Création du DataFrame avec les indices et les erreurs\n",
    "    error_df = pd.DataFrame({\n",
    "        'Latent_Index': indices,\n",
    "        'Reconstruction_Error': errors\n",
    "    })\n",
    "\n",
    "    return error_df\n",
    "error_df=errors_df(cp,X_decoded,scaler)\n",
    "# Fusion avec les clusters UMAP\n",
    "df_umap_clusters_with_errors = pd.merge(df_umap_clusters, error_df, on='Latent_Index', how='left')\n",
    "\n",
    "# Fusion avec les clusters t-SNE\n",
    "df_tsne_clusters_with_errors = pd.merge(df_tsne_clusters, error_df, on='Latent_Index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyenne des erreurs par cluster (UMAP)\n",
    "umap_cluster_errors = df_umap_clusters_with_errors.groupby('UMAP_Cluster')['Reconstruction_Error'].mean().reset_index()\n",
    "umap_cluster_errors = umap_cluster_errors.rename(columns={'Reconstruction_Error': 'Mean_Error'})\n",
    "\n",
    "# Moyenne des erreurs par cluster (t-SNE)\n",
    "tsne_cluster_errors = df_tsne_clusters_with_errors.groupby('tSNE_Cluster')['Reconstruction_Error'].mean().reset_index()\n",
    "tsne_cluster_errors = tsne_cluster_errors.rename(columns={'Reconstruction_Error': 'Mean_Error'})\n",
    "\n",
    "# Affichage des erreurs moyennes par cluster\n",
    "print(\"Erreurs moyennes par cluster (UMAP) :\")\n",
    "print(umap_cluster_errors)\n",
    "\n",
    "print(\"\\nErreurs moyennes par cluster (t-SNE) :\")\n",
    "print(tsne_cluster_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des erreurs moyennes par cluster (UMAP)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(umap_cluster_errors['UMAP_Cluster'], umap_cluster_errors['Mean_Error'])\n",
    "plt.title('Erreurs moyennes de reconstruction par cluster (UMAP)')\n",
    "plt.xlabel('Cluster UMAP')\n",
    "plt.ylabel('Erreur moyenne')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Visualisation des erreurs moyennes par cluster (t-SNE)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(tsne_cluster_errors['tSNE_Cluster'], tsne_cluster_errors['Mean_Error'])\n",
    "plt.title('Erreurs moyennes de reconstruction par cluster (t-SNE)')\n",
    "plt.xlabel('Cluster t-SNE')\n",
    "plt.ylabel('Erreur moyenne')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALISATION DES ERREURS INTRA CLUSTER\n",
    "\n",
    "# Obtenir les clusters uniques et trier\n",
    "clusters = sorted(df_umap_clusters_with_errors['UMAP_Cluster'].unique())\n",
    "\n",
    "# Déterminer le nombre de colonnes et de lignes pour les subplots\n",
    "n_clusters = len(clusters)\n",
    "n_cols = 3  # Nombre de colonnes\n",
    "n_rows = (n_clusters + n_cols - 1) // n_cols  # Calcul du nombre de lignes nécessaires\n",
    "\n",
    "# Initialiser la figure et les subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), constrained_layout=True)\n",
    "axes = axes.flatten()  # Aplatir les axes pour un parcours plus facile\n",
    "\n",
    "# Générer un histogramme pour chaque cluster\n",
    "for i, cluster in enumerate(clusters):\n",
    "    cluster_data = df_umap_clusters_with_errors[df_umap_clusters_with_errors['UMAP_Cluster'] == cluster]\n",
    "    \n",
    "    # Tracer l'histogramme dans le subplot correspondant\n",
    "    axes[i].hist(cluster_data['Reconstruction_Error'], bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f\"Cluster {cluster}\")\n",
    "    axes[i].set_xlabel(\"Reconstruction Error\")\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Supprimer les subplots vides si le nombre de clusters est inférieur au nombre de subplots\n",
    "for j in range(len(clusters), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Ajouter un titre global pour la figure\n",
    "fig.suptitle(\"Distribution of Reconstruction Errors by Cluster\", fontsize=16)\n",
    "\n",
    "# Sauvegarder la figure complète\n",
    "plt.savefig(\"all_clusters_errors.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Les histogrammes combinés ont été générés et sauvegardés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DETECTE LES TRAJECTOIRES AVEC UNE ERREUR DE RECONSTRUCTION TROP GRANDE\n",
    "\n",
    "thresholds = df_umap_clusters_with_errors.groupby('UMAP_Cluster')['Reconstruction_Error'].quantile(0.99)\n",
    "\n",
    "# Ajouter une colonne indiquant si la trajectoire est un outlier\n",
    "df_umap_clusters_with_errors['Is_Outlier'] = df_umap_clusters_with_errors.apply(\n",
    "    lambda row: row['Reconstruction_Error'] > thresholds[row['UMAP_Cluster']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Obtenir les outliers par cluster\n",
    "outliers_by_cluster = {\n",
    "    cluster: df_umap_clusters_with_errors[\n",
    "        (df_umap_clusters_with_errors['UMAP_Cluster'] == cluster) &\n",
    "        (df_umap_clusters_with_errors['Is_Outlier'])\n",
    "    ]['Latent_Index'].tolist()\n",
    "    for cluster in thresholds.index\n",
    "}\n",
    "\n",
    "# Afficher les seuils par cluster\n",
    "print(\"Seuils (99e quantile) par cluster :\")\n",
    "print(thresholds)\n",
    "\n",
    "# Afficher les outliers par cluster\n",
    "print(\"\\nOutliers par cluster :\")\n",
    "for cluster, outliers in outliers_by_cluster.items():\n",
    "    print(f\"Cluster {cluster}: {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_umap_clusters['Dim1'] = df_umap['Dim1']\n",
    "df_umap_clusters['Dim2'] = df_umap['Dim2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AFFCIHE LES OUTLIERS POUR UMAP\n",
    "\n",
    "outlier_indices=[668, 589, 528, 542, 66, 527, 377, 986, 978, 210, 120, 174, 247, 973, 914]\n",
    "\n",
    "\n",
    "df_umap_clusters['Is_Outlier'] = df_umap_clusters['Latent_Index'].isin(outlier_indices)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    df_umap_clusters['Dim1'],\n",
    "    df_umap_clusters['Dim2'],\n",
    "    c=[umap_colors[label] if not is_outlier else 'black' for label, is_outlier in zip(df_umap_clusters['UMAP_Cluster'], df_umap_clusters['Is_Outlier'])],\n",
    "    s=50,\n",
    "    alpha=[0.06 if not is_outlier else 1.0 for is_outlier in df_umap_clusters['Is_Outlier']]  # Transparence pour les non-outliers\n",
    "\n",
    ")\n",
    "plt.title('DBSCAN Clustering (UMAP) avec Outliers')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AFFCIHE LES OUTLIERS POUR PCA\n",
    "\n",
    "highlight_indices = [668, 589, 528, 542, 66, 527, 377, 986, 978, 210, 120, 174, 247, 973, 914]\n",
    "\n",
    "df_pca['Highlight'] = df_pca.index.isin(highlight_indices)\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(\n",
    "    df_pca['Dim1'], \n",
    "    df_pca['Dim2'], \n",
    "    c=[\n",
    "        'black' if highlight else cluster_colors[cluster] \n",
    "        for highlight, cluster in zip(df_pca['Highlight'], df_pca['Cluster'])\n",
    "    ],  # Couleur noire pour les points marqués\n",
    "    s=[\n",
    "        100 if highlight else 50 \n",
    "        for highlight in df_pca['Highlight']\n",
    "    ],  # Points plus grands pour les marqués\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Configuration des axes et du titre\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Projection PCA avec points spécifiques en noir')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
